{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMifndOkGTtZKbdJwd2n0Nn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cs671/workout-progression-predictor/blob/main/ResistanceTraining_Progression_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fogzOaOB0Hqu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f3083572-bc89-43e9-dfbd-215772e249e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèãÔ∏è GOOGLE COLAB WORKOUT MODEL TRAINER üèãÔ∏è\n",
            "============================================================\n",
            "üì¶ STEP 1: Installing required packages...\n",
            "Found existing installation: scikit-learn 1.7.1\n",
            "Uninstalling scikit-learn-1.7.1:\n",
            "  Successfully uninstalled scikit-learn-1.7.1\n",
            "Found existing installation: numpy 2.3.2\n",
            "Uninstalling numpy-2.3.2:\n",
            "  Successfully uninstalled numpy-2.3.2\n",
            "Found existing installation: pandas 2.3.1\n",
            "Uninstalling pandas-2.3.1:\n",
            "  Successfully uninstalled pandas-2.3.1\n",
            "Found existing installation: xgboost 3.0.3\n",
            "Uninstalling xgboost-3.0.3:\n",
            "  Successfully uninstalled xgboost-3.0.3\n",
            "Found existing installation: plotly 6.2.0\n",
            "Uninstalling plotly-6.2.0:\n",
            "  Successfully uninstalled plotly-6.2.0\n",
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Collecting xgboost\n",
            "  Using cached xgboost-3.0.3-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting plotly\n",
            "  Using cached plotly-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting numpy>=1.22.0 (from scikit-learn)\n",
            "  Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: narwhals>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from plotly) (1.48.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "Using cached pandas-2.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "Using cached xgboost-3.0.3-py3-none-manylinux_2_28_x86_64.whl (253.8 MB)\n",
            "Using cached plotly-6.2.0-py3-none-any.whl (9.6 MB)\n",
            "Using cached numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "Installing collected packages: plotly, numpy, pandas, xgboost, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.2 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.3.2 pandas-2.3.1 plotly-6.2.0 scikit-learn-1.7.1 xgboost-3.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pandas",
                  "sklearn"
                ]
              },
              "id": "3d7bcb5343324582a3899dfc908cfca0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Packages installed successfully!\n",
            "Package versions:\n",
            "  scikit-learn: 1.7.1\n",
            "  numpy: 2.3.2\n",
            "  pandas: 2.3.1\n",
            "\n",
            "üìÅ STEP 1.5: Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted successfully!\n",
            "\n",
            "üìÅ STEP 2: Loading data from Google Drive...\n",
            "‚úÖ Loaded: /content/drive/MyDrive/ML Datasets/strong.csv\n",
            "Raw data: 4086 records\n",
            "\n",
            "üßπ STEP 3: Cleaning workout data...\n",
            "Columns: ['Date', 'Workout Name', 'Duration', 'Exercise Name', 'Set Order', 'Weight', 'Reps', 'Distance', 'Seconds', 'Notes', 'Workout Notes', 'RPE']\n",
            "Standardized columns: ['date', 'workout_name', 'duration', 'exercise', 'Set Order', 'weight_kg', 'reps', 'Distance', 'Seconds', 'Notes', 'Workout Notes', 'rpe']\n",
            "Removed 76 invalid records\n",
            "Clean data: 4010 records\n",
            "\n",
            "üìà STEP 4: Creating workout sessions...\n",
            "Created 785 exercise sessions\n",
            "\n",
            "Top exercises for modeling:\n",
            "   Lat Pulldown (Cable): 43 sessions\n",
            "   Seated Row (Cable): 39 sessions\n",
            "   Chest Press (Machine): 38 sessions\n",
            "   Leg Extension (Machine): 37 sessions\n",
            "   Preacher Curl (Machine): 35 sessions\n",
            "   Lying Leg Curl (Machine): 29 sessions\n",
            "   Seated Leg Curl (Machine): 26 sessions\n",
            "   Pec Deck (Machine): 24 sessions\n",
            "\n",
            "üéØ STEP 5: Building progression sequences...\n",
            "   Lat Pulldown (Cable): 60.0kg ‚Üí 72.8kg (+12.8kg)\n",
            "   Seated Row (Cable): 63.0kg ‚Üí 72.0kg (+9.0kg)\n",
            "   Chest Press (Machine): 60.0kg ‚Üí 55.0kg (-5.0kg)\n",
            "   Leg Extension (Machine): 65.0kg ‚Üí 30.0kg (-35.0kg)\n",
            "   Preacher Curl (Machine): 30.0kg ‚Üí 30.0kg (+0.0kg)\n",
            "   Lying Leg Curl (Machine): 39.0kg ‚Üí 40.0kg (+1.0kg)\n",
            "   Seated Leg Curl (Machine): 48.0kg ‚Üí 57.0kg (+9.0kg)\n",
            "   Pec Deck (Machine): 45.0kg ‚Üí 50.0kg (+5.0kg)\n",
            "ML training data: 255 records\n",
            "\n",
            "ü§ñ STEP 6: Preparing features...\n",
            "Exercise encoding:\n",
            "   0: Chest Press (Machine) (36 samples)\n",
            "   1: Lat Pulldown (Cable) (41 samples)\n",
            "   2: Leg Extension (Machine) (35 samples)\n",
            "   3: Lying Leg Curl (Machine) (27 samples)\n",
            "   4: Pec Deck (Machine) (22 samples)\n",
            "   5: Preacher Curl (Machine) (33 samples)\n",
            "   6: Seated Leg Curl (Machine) (24 samples)\n",
            "   7: Seated Row (Cable) (37 samples)\n",
            "Feature matrix: (255, 6)\n",
            "Features: ['prev_weight', 'days_rest', 'prev_rpe_filled', 'prev_volume', 'session_number', 'exercise_encoded']\n",
            "Target range: 20.0 - 90.0 kg\n",
            "\n",
            "üå≤ STEP 7: Training Random Forest model...\n",
            "Training set: 204 examples\n",
            "Test set: 51 examples\n",
            "‚úÖ Model trained successfully!\n",
            "   Test MAE: 7.48 kg\n",
            "\n",
            "üìä Performance by exercise:\n",
            "   Chest Press (Machine): 4.9kg MAE (7 test samples)\n",
            "   Lat Pulldown (Cable): 9.2kg MAE (8 test samples)\n",
            "   Leg Extension (Machine): 11.6kg MAE (7 test samples)\n",
            "   Lying Leg Curl (Machine): 5.2kg MAE (5 test samples)\n",
            "   Pec Deck (Machine): 2.7kg MAE (4 test samples)\n",
            "   Preacher Curl (Machine): 7.1kg MAE (7 test samples)\n",
            "   Seated Leg Curl (Machine): 7.0kg MAE (5 test samples)\n",
            "   Seated Row (Cable): 8.8kg MAE (8 test samples)\n",
            "\n",
            "üéØ Feature Importance:\n",
            "   prev_weight: 0.468\n",
            "   prev_volume: 0.243\n",
            "   exercise_encoded: 0.091\n",
            "   session_number: 0.072\n",
            "   days_rest: 0.067\n",
            "   prev_rpe_filled: 0.058\n",
            "\n",
            "üíæ STEP 8: Saving model files...\n",
            "‚úÖ Saved model files:\n",
            "   best_progression_model.pkl\n",
            "   exercise_label_encoder.pkl\n",
            "   model_info.json\n",
            "\n",
            "üß™ STEP 9: Testing model compatibility...\n",
            "‚úÖ Test prediction: 63.1kg\n",
            "‚úÖ Model is working correctly!\n",
            "\n",
            "üì¶ STEP 10: Creating deployment package...\n",
            "   üì¶ Added best_progression_model.pkl\n",
            "   üì¶ Added exercise_label_encoder.pkl\n",
            "   üì¶ Added model_info.json\n",
            "‚úÖ Created streamlit_deployment.zip\n",
            "\n",
            "üì• STEP 11: Downloading files...\n",
            "Download these files to deploy your Streamlit app:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_04629bd7-ebe6-4c61-b648-9630dd7d8f9d\", \"best_progression_model.pkl\", 1363377)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f73e8e08-102a-4b14-b191-9fa7d6027079\", \"exercise_label_encoder.pkl\", 669)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_590c50af-89a5-4f32-9dc8-29dc2eacb305\", \"model_info.json\", 2416)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_12ec98f1-3549-48ec-9e28-fb0e4dfad491\", \"streamlit_deployment.zip\", 1366846)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üéâ SUCCESS! Your workout AI model is ready!\n",
            "============================================================\n",
            "\n",
            "üìä Model Summary:\n",
            "   Model: Random Forest Regressor\n",
            "   Accuracy: ¬±7.5kg average error\n",
            "   Training samples: 255\n",
            "   Exercises: 8\n",
            "   Best feature: prev_weight\n",
            "\n",
            "üéØ Exercise Coverage:\n",
            "   Chest Press (Machine): 36 training sessions\n",
            "   Lat Pulldown (Cable): 41 training sessions\n",
            "   Leg Extension (Machine): 35 training sessions\n",
            "   Lying Leg Curl (Machine): 27 training sessions\n",
            "   Pec Deck (Machine): 22 training sessions\n",
            "   Preacher Curl (Machine): 33 training sessions\n",
            "   Seated Leg Curl (Machine): 24 training sessions\n",
            "   Seated Row (Cable): 37 training sessions\n",
            "\n",
            "üìã Next Steps for Streamlit Deployment:\n",
            "1. üìÅ Create GitHub repository\n",
            "2. üì§ Upload the 3 downloaded model files\n",
            "3. üìù Add app.py (Streamlit code)\n",
            "4. üìù Add requirements.txt:\n",
            "     streamlit>=1.28.0\n",
            "     pandas>=2.3.1\n",
            "     numpy>=2.3.2\n",
            "     scikit-learn>=1.7.1\n",
            "     xgboost>=2.0.0\n",
            "     joblib>=1.3.0\n",
            "     plotly>=5.15.0\n",
            "5. üöÄ Deploy on share.streamlit.io\n",
            "\n",
            "üí° Your AI can now predict optimal weights for:\n",
            "   üèãÔ∏è Chest Press (Machine)\n",
            "   üèãÔ∏è Lat Pulldown (Cable)\n",
            "   üèãÔ∏è Leg Extension (Machine)\n",
            "   üèãÔ∏è Lying Leg Curl (Machine)\n",
            "   üèãÔ∏è Pec Deck (Machine)\n",
            "   ... and 3 more exercises!\n",
            "\n",
            "üéä Your personalized workout AI is ready for deployment!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Workout Progression Predictor - Model Training Pipeline\n",
        "Author: Daniel Romeo\n",
        "Date: 02/08/2025\n",
        "\n",
        "This script trains a Random Forest model to predict optimal workout progressions\n",
        "based on personal training data from the Strong fitness app.\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Any\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import joblib\n",
        "import json\n",
        "import warnings\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Suppress sklearn warnings\n",
        "warnings.filterwarnings('ignore', message='X does not have valid feature names')\n",
        "warnings.filterwarnings('ignore', category=RuntimeWarning, module='pandas')\n",
        "\n",
        "\n",
        "class WorkoutDataProcessor:\n",
        "    \"\"\"Handles data loading, cleaning, and feature engineering for workout data.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.column_mapping = {\n",
        "            'Exercise Name': 'exercise',\n",
        "            'Date': 'date',\n",
        "            'Weight': 'weight_kg',\n",
        "            'Reps': 'reps',\n",
        "            'RPE': 'rpe',\n",
        "            'Workout Name': 'workout_name',\n",
        "            'Duration': 'duration'\n",
        "        }\n",
        "\n",
        "    def load_and_clean_data(self, filepath: str) -> pd.DataFrame:\n",
        "        \"\"\"Load and clean raw workout data.\"\"\"\n",
        "        logger.info(f\"Loading data from {filepath}\")\n",
        "\n",
        "        df = pd.read_csv(filepath)\n",
        "        logger.info(f\"Loaded {len(df)} raw records\")\n",
        "\n",
        "        # Standardize columns\n",
        "        df = df.rename(columns=self.column_mapping)\n",
        "\n",
        "        # Clean data types\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "        numeric_cols = ['weight_kg', 'reps', 'rpe']\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Remove invalid records\n",
        "        initial_rows = len(df)\n",
        "        df = df.dropna(subset=['date', 'exercise', 'weight_kg', 'reps'])\n",
        "        df = df[(df['weight_kg'] > 0) & (df['reps'] > 0)].copy()\n",
        "        df['volume'] = df['weight_kg'] * df['reps']\n",
        "\n",
        "        logger.info(f\"Cleaned data: {len(df)} records ({initial_rows - len(df)} removed)\")\n",
        "        return df\n",
        "\n",
        "    def create_sessions(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Aggregate individual sets into workout sessions.\"\"\"\n",
        "        df['session_date'] = df['date'].dt.date\n",
        "\n",
        "        sessions = df.groupby(['session_date', 'exercise']).agg({\n",
        "            'weight_kg': 'max',\n",
        "            'volume': 'sum',\n",
        "            'reps': 'mean',\n",
        "            'rpe': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        sessions['date'] = pd.to_datetime(sessions['session_date'])\n",
        "        sessions = sessions.sort_values(['exercise', 'date'])\n",
        "\n",
        "        logger.info(f\"Created {len(sessions)} exercise sessions\")\n",
        "        return sessions\n",
        "\n",
        "    def filter_exercises(self, sessions: pd.DataFrame, min_sessions: int = 10, max_exercises: int = 8) -> pd.DataFrame:\n",
        "        \"\"\"Filter to exercises with sufficient data for modeling.\"\"\"\n",
        "        exercise_counts = sessions['exercise'].value_counts()\n",
        "        top_exercises = exercise_counts[exercise_counts >= min_sessions].head(max_exercises)\n",
        "\n",
        "        filtered_sessions = sessions[sessions['exercise'].isin(top_exercises.index)]\n",
        "\n",
        "        logger.info(f\"Selected {len(top_exercises)} exercises for modeling:\")\n",
        "        for exercise, count in top_exercises.items():\n",
        "            logger.info(f\"  {exercise}: {count} sessions\")\n",
        "\n",
        "        return filtered_sessions\n",
        "\n",
        "\n",
        "class ProgressionFeatureEngineer:\n",
        "    \"\"\"Creates features for progression modeling.\"\"\"\n",
        "\n",
        "    def create_progression_features(self, sessions: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create lag features and progression sequences.\"\"\"\n",
        "        progression_data = []\n",
        "\n",
        "        for exercise in sessions['exercise'].unique():\n",
        "            ex_data = sessions[sessions['exercise'] == exercise].copy()\n",
        "            ex_data = ex_data.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "            if len(ex_data) < 3:\n",
        "                continue\n",
        "\n",
        "            # Create lag features\n",
        "            ex_data['prev_weight'] = ex_data['weight_kg'].shift(1)\n",
        "            ex_data['prev_rpe'] = ex_data['rpe'].shift(1)\n",
        "            ex_data['prev_volume'] = ex_data['volume'].shift(1)\n",
        "            ex_data['days_rest'] = ex_data['date'].diff().dt.days\n",
        "            ex_data['next_weight'] = ex_data['weight_kg'].shift(-1)\n",
        "            ex_data['session_number'] = range(1, len(ex_data) + 1)\n",
        "\n",
        "            progression_data.append(ex_data)\n",
        "\n",
        "            # Log progression summary\n",
        "            start_weight = ex_data['weight_kg'].iloc[0]\n",
        "            end_weight = ex_data['weight_kg'].iloc[-1]\n",
        "            total_change = end_weight - start_weight\n",
        "            logger.info(f\"  {exercise}: {start_weight:.1f}kg ‚Üí {end_weight:.1f}kg ({total_change:+.1f}kg)\")\n",
        "\n",
        "        if not progression_data:\n",
        "            raise ValueError(\"No exercises found with sufficient data\")\n",
        "\n",
        "        ml_data = pd.concat(progression_data, ignore_index=True)\n",
        "\n",
        "        # Remove sessions without complete features/targets\n",
        "        valid_data = ml_data[\n",
        "            ml_data['prev_weight'].notna() &\n",
        "            ml_data['next_weight'].notna()\n",
        "        ].copy()\n",
        "\n",
        "        logger.info(f\"Created {len(valid_data)} training examples\")\n",
        "        return valid_data\n",
        "\n",
        "    def prepare_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series, LabelEncoder]:\n",
        "        \"\"\"Prepare final feature matrix and target vector.\"\"\"\n",
        "        # Encode exercises\n",
        "        label_encoder = LabelEncoder()\n",
        "        data['exercise_encoded'] = label_encoder.fit_transform(data['exercise'])\n",
        "\n",
        "        # Handle missing RPE values\n",
        "        data['prev_rpe_filled'] = data.groupby('exercise')['prev_rpe'].transform(\n",
        "            lambda x: x.fillna(x.median())\n",
        "        )\n",
        "\n",
        "        # Fill remaining missing values\n",
        "        overall_rpe_median = data['rpe'].median()\n",
        "        data['prev_rpe_filled'] = data['prev_rpe_filled'].fillna(overall_rpe_median)\n",
        "        data['days_rest'] = data['days_rest'].fillna(7.0)\n",
        "        data['prev_volume'] = data['prev_volume'].fillna(data['prev_volume'].median())\n",
        "\n",
        "        # Create feature matrix\n",
        "        feature_columns = [\n",
        "            'prev_weight',\n",
        "            'days_rest',\n",
        "            'prev_rpe_filled',\n",
        "            'prev_volume',\n",
        "            'session_number',\n",
        "            'exercise_encoded'\n",
        "        ]\n",
        "\n",
        "        X = data[feature_columns].copy()\n",
        "        y = data['next_weight'].copy()\n",
        "\n",
        "        logger.info(f\"Feature matrix: {X.shape}, Target range: {y.min():.1f} - {y.max():.1f} kg\")\n",
        "        return X, y, label_encoder\n",
        "\n",
        "\n",
        "class WorkoutProgressionModel:\n",
        "    \"\"\"Random Forest model for workout progression prediction.\"\"\"\n",
        "\n",
        "    def __init__(self, **model_params):\n",
        "        default_params = {\n",
        "            'n_estimators': 200,\n",
        "            'max_depth': 15,\n",
        "            'min_samples_split': 5,\n",
        "            'min_samples_leaf': 2,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        default_params.update(model_params)\n",
        "        self.model = RandomForestRegressor(**default_params)\n",
        "\n",
        "    def train(self, X: pd.DataFrame, y: pd.Series, test_size: float = 0.2) -> Dict[str, Any]:\n",
        "        \"\"\"Train model and return performance metrics.\"\"\"\n",
        "        # Stratified split by exercise\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42,\n",
        "            stratify=X['exercise_encoded']\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Training on {len(X_train)} examples, testing on {len(X_test)}\")\n",
        "\n",
        "        # Train model\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = self.model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "        logger.info(f\"Model trained. Test MAE: {mae:.2f} kg\")\n",
        "\n",
        "        return {\n",
        "            'model': self.model,\n",
        "            'mae': mae,\n",
        "            'X_test': X_test,\n",
        "            'y_test': y_test,\n",
        "            'y_pred': y_pred\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_exercise_performance(results: Dict, label_encoder: LabelEncoder) -> pd.DataFrame:\n",
        "    \"\"\"Calculate exercise-specific performance metrics.\"\"\"\n",
        "    test_data = results['X_test'].copy()\n",
        "    test_data['actual'] = results['y_test'].values\n",
        "    test_data['predicted'] = results['y_pred']\n",
        "    test_data['exercise'] = label_encoder.inverse_transform(results['X_test']['exercise_encoded'])\n",
        "\n",
        "    exercise_performance = test_data.groupby('exercise').apply(\n",
        "        lambda x: pd.Series({\n",
        "            'samples': len(x),\n",
        "            'mae': mean_absolute_error(x['actual'], x['predicted']),\n",
        "            'mean_actual': x['actual'].mean(),\n",
        "            'mean_predicted': x['predicted'].mean()\n",
        "        }), include_groups=False\n",
        "    ).round(2)\n",
        "\n",
        "    return exercise_performance\n",
        "\n",
        "\n",
        "def save_model_artifacts(model: RandomForestRegressor, label_encoder: LabelEncoder,\n",
        "                        mae: float, feature_columns: list, exercise_performance: pd.DataFrame,\n",
        "                        output_dir: str = '.'):\n",
        "    \"\"\"Save trained model and metadata.\"\"\"\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # Save model and encoder\n",
        "    joblib.dump(model, output_path / 'best_progression_model.pkl')\n",
        "    joblib.dump(label_encoder, output_path / 'exercise_label_encoder.pkl')\n",
        "\n",
        "    # Create feature importance analysis\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_columns,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    # Save comprehensive metadata\n",
        "    model_info = {\n",
        "        'model_type': 'Random Forest Regressor',\n",
        "        'mae': float(mae),\n",
        "        'features': feature_columns,\n",
        "        'feature_importance': feature_importance.set_index('feature')['importance'].to_dict(),\n",
        "        'exercises': list(label_encoder.classes_),\n",
        "        'exercise_performance': exercise_performance.to_dict('index'),\n",
        "        'package_versions': {\n",
        "            'scikit_learn': __import__('sklearn').__version__,\n",
        "            'numpy': np.__version__,\n",
        "            'pandas': pd.__version__,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(output_path / 'model_info.json', 'w') as f:\n",
        "        json.dump(model_info, f, indent=2)\n",
        "\n",
        "    logger.info(f\"Model artifacts saved to {output_path}\")\n",
        "    return feature_importance\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main training pipeline.\"\"\"\n",
        "    # Configuration\n",
        "    DATA_PATH = \"/content/drive/MyDrive/ML Datasets/strong.csv\"  # Update path as needed\n",
        "    OUTPUT_DIR = \".\"\n",
        "\n",
        "    try:\n",
        "        # Initialize processors\n",
        "        processor = WorkoutDataProcessor()\n",
        "        feature_engineer = ProgressionFeatureEngineer()\n",
        "\n",
        "        # Load and process data\n",
        "        raw_data = processor.load_and_clean_data(DATA_PATH)\n",
        "        sessions = processor.create_sessions(raw_data)\n",
        "        filtered_sessions = processor.filter_exercises(sessions)\n",
        "\n",
        "        # Engineer features\n",
        "        progression_data = feature_engineer.create_progression_features(filtered_sessions)\n",
        "        X, y, label_encoder = feature_engineer.prepare_features(progression_data)\n",
        "\n",
        "        # Train model\n",
        "        model_trainer = WorkoutProgressionModel()\n",
        "        results = model_trainer.train(X, y)\n",
        "\n",
        "        # Evaluate performance\n",
        "        exercise_performance = evaluate_exercise_performance(results, label_encoder)\n",
        "        logger.info(\"Exercise-specific performance:\")\n",
        "        for exercise in exercise_performance.index:\n",
        "            perf = exercise_performance.loc[exercise]\n",
        "            logger.info(f\"  {exercise}: {perf['mae']:.1f}kg MAE ({int(perf['samples'])} samples)\")\n",
        "\n",
        "        # Save artifacts\n",
        "        feature_importance = save_model_artifacts(\n",
        "            results['model'], label_encoder, results['mae'],\n",
        "            X.columns.tolist(), exercise_performance, OUTPUT_DIR\n",
        "        )\n",
        "\n",
        "        logger.info(\"Training pipeline completed successfully!\")\n",
        "        logger.info(f\"Final model MAE: {results['mae']:.2f} kg\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Training pipeline failed: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}